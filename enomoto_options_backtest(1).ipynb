{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enomoto Options Strategy Backtesting Framework\n",
    "\n",
    "This notebook implements the quantitative analysis framework of financial analyst Josh Enomoto.\n",
    "\n",
    "## Framework Overview\n",
    "\n",
    "The goal is to identify statistically mispriced options by finding a \"positive delta in price density dynamics\" - the gap between:\n",
    "- **Baseline**: Normal price behavior across all historical data\n",
    "- **Sequence-Specific**: Price behavior after a specific 10-week pattern\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Morse Code Pattern (X-Y-Z)**:\n",
    "   - X = Up weeks (Close > Open)\n",
    "   - Y = Down weeks (Close <= Open)\n",
    "   - Z = Trajectory ('D' if downward, 'U' if upward)\n",
    "\n",
    "2. **Data Period**: Weekly data from January 1, 2019 onwards\n",
    "\n",
    "3. **Price Points**:\n",
    "   - Anchor Price: Open of the first week in the 10-week window\n",
    "   - Resolution Price: Low price at a future week (e.g., 8 weeks out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:16:21.124967800Z",
     "start_time": "2025-11-03T00:16:20.610955300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core data manipulation and numerical libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Data acquisition\n",
    "import yfinance as yf\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import gaussian_kde, binomtest\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Acquisition Function\n",
    "\n",
    "Download weekly historical data from January 1, 2019 to present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:21:36.922633300Z",
     "start_time": "2025-11-03T00:21:36.911146Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_weekly_data(ticker, start_date='2019-01-01'):\n",
    "    \"\"\"\n",
    "    Download weekly historical data for a given ticker.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ticker : str\n",
    "        Stock ticker symbol\n",
    "    start_date : str\n",
    "        Start date for historical data (default: '2019-01-01')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with weekly OHLC data\n",
    "    \"\"\"\n",
    "    print(f\"\\nDownloading weekly data for {ticker} from {start_date}...\")\n",
    "    \n",
    "    # Download data with weekly interval\n",
    "    data = yf.download(ticker, start=start_date, interval='1wk', progress=False)\n",
    "    \n",
    "    # --- START FIX ---\n",
    "    # Handle potential MultiIndex columns returned by yfinance\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        # Assumes the structure is ('Open', 'KDP'), ('Close', 'KDP')\n",
    "        # We drop the second level (ticker) to get clean columns\n",
    "        data.columns = data.columns.droplevel(1)\n",
    "    # --- END FIX ---\n",
    "\n",
    "    # Reset index to make Date a column\n",
    "    data = data.reset_index()\n",
    "    \n",
    "    # Ensure we have the necessary columns\n",
    "    required_cols = ['Date', 'Open', 'Close', 'Low', 'High']\n",
    "    for col in required_cols:\n",
    "        if col not in data.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    \n",
    "    print(f\"Downloaded {len(data)} weeks of data\")\n",
    "    print(f\"Date range: {data['Date'].min()} to {data['Date'].max()}\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering - The \"Morse Code\" Pattern\n",
    "\n",
    "Implement the pattern detection logic that identifies 10-week sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:16:28.597306100Z",
     "start_time": "2025-11-03T00:16:28.566150600Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sequence(window):\n",
    "    \"\"\"\n",
    "    Calculate the 'Morse Code' pattern for a 10-week window.\n",
    "    \n",
    "    Pattern format: \"X-Y-Z\" where:\n",
    "    - X = number of up weeks (Close > Open)\n",
    "    - Y = number of down weeks (Close <= Open)\n",
    "    - Z = trajectory ('D' for downward, 'U' for upward)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    window : pd.DataFrame\n",
    "        10-week DataFrame window with 'Open' and 'Close' columns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Pattern string in format \"X-Y-Z\" (e.g., \"3-7-D\")\n",
    "    \"\"\"\n",
    "    # Count up weeks (Close > Open)\n",
    "    up_weeks = (window['Close'] > window['Open']).sum()\n",
    "    \n",
    "    # Count down weeks (Close <= Open)\n",
    "    down_weeks = (window['Close'] <= window['Open']).sum()\n",
    "    \n",
    "    # Determine trajectory: Compare last Close to first Open\n",
    "    # 'D' if closing price of 10th week < opening price of 1st week\n",
    "    # Use .values to ensure we get scalar values, not Series\n",
    "    trajectory = 'D' if window['Close'].values[-1] < window['Open'].values[0] else 'U'\n",
    "    \n",
    "    # Format as \"X-Y-Z\"\n",
    "    sequence_key = f\"{up_weeks}-{down_weeks}-{trajectory}\"\n",
    "    \n",
    "    return sequence_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Historical Pattern Analysis - Rolling Window\n",
    "\n",
    "Create a master DataFrame that analyzes all 10-week windows and their future outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:16:32.407461500Z",
     "start_time": "2025-11-03T00:16:32.391280100Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_master_dataframe(data, window_size=10, future_weeks=10):\n",
    "    \"\"\"\n",
    "    Build master analysis DataFrame with all 10-week windows and their future outcomes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Historical weekly data\n",
    "    window_size : int\n",
    "        Size of the rolling window (default: 10)\n",
    "    future_weeks : int\n",
    "        Number of future weeks to track (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Master DataFrame with pattern analysis\n",
    "    \"\"\"\n",
    "    print(f\"\\nBuilding master DataFrame with {window_size}-week rolling windows...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Iterate through all possible 10-week windows\n",
    "    for i in range(len(data) - window_size - future_weeks + 1):\n",
    "        # Extract the 10-week window\n",
    "        window = data.iloc[i:i + window_size]\n",
    "        \n",
    "        # Calculate pattern sequence\n",
    "        sequence_key = get_sequence(window)\n",
    "        \n",
    "        # Get anchor price (Open of first week)\n",
    "        # Use .values to ensure we get scalar values, not Series\n",
    "        anchor_price = window['Open'].values[0]\n",
    "        \n",
    "        # Get start and end dates\n",
    "        start_date = window['Date'].values[0]\n",
    "        end_date = window['Date'].values[-1]\n",
    "        \n",
    "        # Build result row\n",
    "        row = {\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'anchor_price': anchor_price,\n",
    "            'sequence_key': sequence_key\n",
    "        }\n",
    "        \n",
    "        # Add future resolution prices (Low prices for next N weeks)\n",
    "        for week_offset in range(1, future_weeks + 1):\n",
    "            future_idx = i + window_size + week_offset - 1\n",
    "            if future_idx < len(data):\n",
    "                row[f'future_week_{week_offset}_low'] = data['Low'].values[future_idx]\n",
    "            else:\n",
    "                row[f'future_week_{week_offset}_low'] = np.nan\n",
    "        \n",
    "        results.append(row)\n",
    "    \n",
    "    master_df = pd.DataFrame(results)\n",
    "    print(f\"Created master DataFrame with {len(master_df)} windows\")\n",
    "    print(f\"Found {master_df['sequence_key'].nunique()} unique patterns\")\n",
    "    \n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Distribution Analysis and Metric Calculation\n",
    "\n",
    "Calculate key statistical metrics including clustering, delta, and p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:16:35.341112900Z",
     "start_time": "2025-11-03T00:16:35.337084400Z"
    }
   },
   "outputs": [],
   "source": "def calculate_clustering_price(data, bandwidth='scott'):\n    \"\"\"\n    Calculate the clustering price (mode/peak) of a distribution using KDE.\n    \n    Parameters:\n    -----------\n    data : pd.Series or np.array\n        Price data\n    bandwidth : str or float\n        KDE bandwidth method (default: 'scott')\n    \n    Returns:\n    --------\n    float\n        The price at the peak of the KDE distribution\n    \"\"\"\n    # Remove NaN values and ensure proper 1D numpy array format\n    if isinstance(data, pd.Series):\n        clean_data = data.dropna().values\n    else:\n        clean_data = np.array(data)\n        clean_data = clean_data[~np.isnan(clean_data)]\n    \n    # Ensure 1D array\n    clean_data = clean_data.flatten()\n    \n    if len(clean_data) < 2:\n        return np.nan\n    \n    # Fit KDE\n    kde = gaussian_kde(clean_data, bw_method=bandwidth)\n    \n    # Create evaluation grid\n    x_grid = np.linspace(clean_data.min(), clean_data.max(), 1000)\n    kde_values = kde(x_grid)\n    \n    # Find peak (mode)\n    peak_idx = np.argmax(kde_values)\n    clustering_price = x_grid[peak_idx]\n    \n    return clustering_price\n\n\ndef calculate_metrics(master_df, target_sequence_key, target_resolution_week, current_anchor_price, baseline_lookback_weeks):\n    \"\"\"\n    Calculate all core Enomoto metrics using pooled data from weeks 1 to target_resolution_week.\n    \n    Parameters:\n    -----------\n    master_df : pd.DataFrame\n        Master analysis DataFrame\n    target_sequence_key : str\n        Target pattern (e.g., '3-7-D')\n    target_resolution_week : int\n        Target resolution week (e.g., 4, 8, or 9)\n    current_anchor_price : float\n        Current anchor price for exceedance calculation\n    baseline_lookback_weeks : int\n        Number of recent weeks to use for baseline calculation\n    \n    Returns:\n    --------\n    dict\n        Dictionary with all calculated metrics\n    \"\"\"\n    # Create list of all future column names from week 1 up to target week\n    future_cols = [f'future_week_{i}_low' for i in range(1, target_resolution_week + 1)]\n    \n    # --- START FIX: Calculate baseline from recent lookback only ---\n    # Get the most recent N windows for the baseline calculation\n    baseline_df = master_df.tail(baseline_lookback_weeks)\n    # Pool all future data from that recent period\n    baseline_data = baseline_df[future_cols].stack().dropna()\n    # --- END FIX ---\n    \n    # Create pooled sequence data: stack all columns for matching sequences\n    sequence_mask = master_df['sequence_key'] == target_sequence_key\n    sequence_data = master_df.loc[sequence_mask, future_cols].stack().dropna()\n    \n    if len(sequence_data) == 0:\n        print(f\"WARNING: No data found for sequence '{target_sequence_key}'\")\n        return None\n    \n    # 1. Calculate clustering prices (mode of KDE)\n    baseline_clustering = calculate_clustering_price(baseline_data)\n    sequence_clustering = calculate_clustering_price(sequence_data)\n    \n    # 2. Positive Delta\n    positive_delta = (sequence_clustering - baseline_clustering) / baseline_clustering\n    \n    # 3. Pattern Rarity\n    pattern_rarity = len(sequence_data) / len(baseline_data)\n    \n    # 4. Exceedance Ratio (sequence-specific)\n    exceedance_ratio = (sequence_data > current_anchor_price).mean()\n    \n    # 5. Terminal Median\n    terminal_median = sequence_data.median()\n    \n    # 6. P-Value (Binomial Test)\n    baseline_success_rate = (baseline_data > current_anchor_price).mean()\n    sequence_success_count = (sequence_data > current_anchor_price).sum()\n    n_trials = len(sequence_data)\n    \n    # Perform binomial test\n    binom_result = binomtest(sequence_success_count, n=n_trials, p=baseline_success_rate, alternative='greater')\n    p_value = binom_result.pvalue\n    \n    # Compile metrics\n    metrics = {\n        'baseline_clustering': baseline_clustering,\n        'sequence_clustering': sequence_clustering,\n        'positive_delta': positive_delta,\n        'positive_delta_pct': positive_delta * 100,\n        'pattern_rarity': pattern_rarity,\n        'pattern_rarity_pct': pattern_rarity * 100,\n        'exceedance_ratio': exceedance_ratio,\n        'exceedance_ratio_pct': exceedance_ratio * 100,\n        'terminal_median': terminal_median,\n        'p_value': p_value,\n        'baseline_success_rate': baseline_success_rate,\n        'sequence_success_count': sequence_success_count,\n        'n_trials': n_trials,\n        'baseline_median': baseline_data.median(),\n        'sequence_median': sequence_data.median(),\n        'baseline_count': len(baseline_data),\n        'sequence_count': len(sequence_data)\n    }\n    \n    return metrics, baseline_data, sequence_data"
  },
  {
   "cell_type": "code",
   "source": "def analyze_weekly_resolution_profile(master_df, target_sequence_key, current_anchor_price, baseline_lookback_weeks, weeks_range=(1, 10)):\n    \"\"\"\n    Analyze all resolution weeks (1-10) to find the optimal week with the strongest pattern resolution.\n    \n    Calculates three key metrics for each week:\n    1. Signal divergence: abs(exceedance_ratio - baseline_success_rate)\n    2. Clustering stability: 1 / (std_dev_of_sequence_data + 1)\n    3. Exceedance strength: exceedance_ratio value\n    \n    Computes composite score: (divergence * 0.4) + (stability * 0.3) + (exceedance * 0.3)\n    \n    Parameters:\n    -----------\n    master_df : pd.DataFrame\n        Master analysis DataFrame\n    target_sequence_key : str\n        Target pattern (e.g., '3-7-D')\n    current_anchor_price : float\n        Current anchor price for exceedance calculation\n    baseline_lookback_weeks : int\n        Number of recent weeks to use for baseline calculation\n    weeks_range : tuple\n        Range of weeks to analyze (default: (1, 10))\n    \n    Returns:\n    --------\n    tuple\n        (weekly_analysis_df, optimal_week)\n        - weekly_analysis_df: DataFrame with scores for each week\n        - optimal_week: Week number with highest composite score\n    \"\"\"\n    print(\"\\nAnalyzing weekly resolution profile (weeks 1-10)...\")\n    \n    results = []\n    \n    for week in range(weeks_range[0], weeks_range[1] + 1):\n        # Calculate metrics for this week\n        result = calculate_metrics(master_df, target_sequence_key, week, current_anchor_price, baseline_lookback_weeks)\n        \n        if result is None:\n            continue\n            \n        metrics, baseline_data, sequence_data = result\n        \n        # Calculate the three key components\n        # 1. Signal divergence: How different is the sequence from baseline?\n        signal_divergence = abs(metrics['exceedance_ratio'] - metrics['baseline_success_rate'])\n        \n        # 2. Clustering stability: How consistent is the sequence data? (inverse of std dev)\n        sequence_std = sequence_data.std()\n        clustering_stability = 1.0 / (sequence_std + 1.0)\n        \n        # 3. Exceedance strength: Raw probability of exceeding anchor price\n        exceedance_strength = metrics['exceedance_ratio']\n        \n        # Composite score with specified weights\n        composite_score = (signal_divergence * 0.4) + (clustering_stability * 0.3) + (exceedance_strength * 0.3)\n        \n        results.append({\n            'week': week,\n            'composite_score': composite_score,\n            'signal_divergence': signal_divergence,\n            'clustering_stability': clustering_stability,\n            'exceedance_ratio': metrics['exceedance_ratio'],\n            'p_value': metrics['p_value'],\n            'sequence_count': metrics['sequence_count']\n        })\n    \n    # Create DataFrame\n    weekly_df = pd.DataFrame(results)\n    \n    # Find optimal week (highest composite score)\n    optimal_idx = weekly_df['composite_score'].idxmax()\n    optimal_week = int(weekly_df.loc[optimal_idx, 'week'])\n    \n    print(f\"Optimal resolution week identified: Week {optimal_week}\")\n    \n    return weekly_df, optimal_week",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Chart Generation\n",
    "\n",
    "Create verification charts: Baseline distribution and bimodal comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:16:38.823776800Z",
     "start_time": "2025-11-03T00:16:38.807935800Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_baseline_distribution(ticker, baseline_data):\n",
    "    \"\"\"\n",
    "    Plot Chart 1: Simple baseline distribution with median line.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ticker : str\n",
    "        Stock ticker\n",
    "    baseline_data : pd.Series\n",
    "        Baseline price data\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot KDE\n",
    "    sns.kdeplot(data=baseline_data, ax=ax, color='steelblue', linewidth=2, fill=True, alpha=0.3)\n",
    "    \n",
    "    # Add median line\n",
    "    median_val = baseline_data.median()\n",
    "    ax.axvline(median_val, color='red', linestyle='--', linewidth=2, label=f'Median: ${median_val:.2f}')\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel('Price ($)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Density', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{ticker} - Baseline Distribution (All Outcomes)', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_bimodal_distribution(ticker, target_sequence_key, baseline_data, sequence_data):\n",
    "    \"\"\"\n",
    "    Plot Chart 2: Bimodal/juxtaposed distribution with GMM overlay.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ticker : str\n",
    "        Stock ticker\n",
    "    target_sequence_key : str\n",
    "        Target pattern\n",
    "    baseline_data : pd.Series\n",
    "        Baseline price data\n",
    "    sequence_data : pd.Series\n",
    "        Sequence-specific price data\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "    # Layer 1: Baseline Distribution KDE\n",
    "    sns.kdeplot(data=baseline_data, ax=ax, color='green', linewidth=2, \n",
    "                fill=True, alpha=0.3, label='Baseline (All Outcomes)')\n",
    "    \n",
    "    # Layer 2: Sequence-Specific Distribution KDE\n",
    "    sns.kdeplot(data=sequence_data, ax=ax, color='blue', linewidth=2.5, \n",
    "                fill=False, alpha=0.8, label=f'Sequence-Specific ({target_sequence_key})')\n",
    "    \n",
    "    # Layer 3: Gaussian Mixture Model (Bimodal)\n",
    "    if len(sequence_data) >= 2:\n",
    "        try:\n",
    "            # Fit GMM with 2 components\n",
    "            gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "            gmm.fit(sequence_data.values.reshape(-1, 1))\n",
    "            \n",
    "            # Generate PDF for GMM\n",
    "            x_range = np.linspace(sequence_data.min(), sequence_data.max(), 1000)\n",
    "            \n",
    "            # Calculate GMM PDF\n",
    "            logprob = gmm.score_samples(x_range.reshape(-1, 1))\n",
    "            pdf = np.exp(logprob)\n",
    "            \n",
    "            # Plot GMM\n",
    "            ax.plot(x_range, pdf, color='red', linestyle='--', linewidth=2, \n",
    "                   label='Bimodal (GMM)', alpha=0.8)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not fit GMM - {e}\")\n",
    "    \n",
    "    # Add median lines\n",
    "    baseline_median = baseline_data.median()\n",
    "    sequence_median = sequence_data.median()\n",
    "    \n",
    "    ax.axvline(baseline_median, color='green', linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "    ax.axvline(sequence_median, color='blue', linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel('Price ($)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Density', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{ticker} - Bimodal Distribution (Sequence: {target_sequence_key})', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10, loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: Main Analysis Function\n\nIntegrate all components into a single analysis pipeline with automatic optimal week selection."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:16:42.363457800Z",
     "start_time": "2025-11-03T00:16:42.244873600Z"
    }
   },
   "outputs": [],
   "source": "def run_enomoto_analysis(ticker, target_resolution_week=8, \n                         baseline_lookback_weeks=104,\n                         start_date='2019-01-01', generate_charts=True,\n                         auto_select_week=True):\n    \"\"\"\n    Run complete Enomoto analysis for a given ticker.\n    Automatically detects the current pattern from the most recent 10-week window.\n    \n    Parameters:\n    -----------\n    ticker : str\n        Stock ticker symbol\n    target_resolution_week : int\n        Future week for resolution price (default: 8, used when auto_select_week=False)\n    baseline_lookback_weeks : int\n        Number of recent weeks to use for baseline calculation (default: 104)\n    start_date : str\n        Start date for historical data (default: '2019-01-01')\n    generate_charts : bool\n        Whether to generate verification charts (default: True)\n    auto_select_week : bool\n        Whether to automatically select optimal resolution week (default: True)\n    \n    Returns:\n    --------\n    dict\n        Dictionary with all metrics and data\n    \"\"\"\n    print(\"=\"*80)\n    print(f\"ENOMOTO ANALYSIS: {ticker}\")\n    print(\"=\"*80)\n    \n    # Step 1: Download data\n    data = download_weekly_data(ticker, start_date)\n    \n    # Step 2: Get current 10-week window and calculate target sequence\n    print(\"\\nCalculating current pattern from most recent 10 weeks...\")\n    if len(data) < 10:\n        print(\"ERROR: Not enough data. Need at least 10 weeks.\")\n        return None\n    \n    current_window = data.iloc[-10:]\n    target_sequence_key = get_sequence(current_window)\n    \n    print(f\"Detected Pattern: {target_sequence_key}\")\n    \n    # Step 3: Build master DataFrame\n    master_df = build_master_dataframe(data)\n    \n    # Step 4: Get current anchor price (most recent 10-week window)\n    current_anchor_price = float(master_df['anchor_price'].iloc[-1])\n    print(f\"Current anchor price: ${current_anchor_price:.2f}\")\n    \n    # Step 5: Analyze weekly resolution profile (if auto_select_week is True)\n    weekly_analysis_df = None\n    optimal_week = target_resolution_week\n    \n    if auto_select_week:\n        weekly_analysis_df, optimal_week = analyze_weekly_resolution_profile(\n            master_df, target_sequence_key, current_anchor_price, baseline_lookback_weeks\n        )\n        \n        # Print weekly breakdown\n        print(\"\\n\" + \"=\"*80)\n        print(\"WEEKLY RESOLUTION PROFILE\")\n        print(\"=\"*80)\n        print(f\"{'Week':<6} {'Score':<8} {'Divergence':<12} {'Exceedance':<12} {'P-Value':<10} {'Optimal':<8}\")\n        print(\"-\"*80)\n        \n        for _, row in weekly_analysis_df.iterrows():\n            week_num = int(row['week'])\n            marker = \"★\" if week_num == optimal_week else \"\"\n            print(f\"{week_num:<6} {row['composite_score']:<8.4f} {row['signal_divergence']:<12.4f} \"\n                  f\"{row['exceedance_ratio']:<12.4f} {row['p_value']:<10.6f} {marker:<8}\")\n        \n        print(\"=\"*80)\n        print(f\"\\nUsing optimal resolution week: {optimal_week}\")\n    else:\n        print(f\"\\nUsing fixed resolution week: {target_resolution_week}\")\n    \n    print(\"=\"*80)\n    \n    # Step 6: Calculate metrics using the selected resolution week\n    print(\"\\nCalculating final metrics...\")\n    result = calculate_metrics(master_df, target_sequence_key, optimal_week, current_anchor_price, baseline_lookback_weeks)\n    \n    if result is None:\n        print(\"Analysis failed - no data for target sequence\")\n        return None\n    \n    metrics, baseline_data, sequence_data = result\n    \n    # Step 7: Print metrics\n    print(\"\\n\" + \"=\"*80)\n    print(\"RESULTS\")\n    print(\"=\"*80)\n    print(f\"\\n1. PRICE CLUSTERING:\")\n    print(f\"   Baseline Clustering:          ${metrics['baseline_clustering']:.2f}\")\n    print(f\"   Sequence Clustering:          ${metrics['sequence_clustering']:.2f}\")\n    print(f\"\\n2. POSITIVE DELTA:              {metrics['positive_delta_pct']:.2f}%\")\n    print(f\"\\n3. PATTERN RARITY:              {metrics['pattern_rarity_pct']:.2f}% ({metrics['sequence_count']}/{metrics['baseline_count']} windows)\")\n    print(f\"\\n4. EXCEEDANCE RATIO:            {metrics['exceedance_ratio_pct']:.2f}%\")\n    print(f\"   (Probability resolution > anchor)\")\n    print(f\"\\n5. TERMINAL MEDIAN:             ${metrics['terminal_median']:.2f}\")\n    print(f\"\\n6. STATISTICAL SIGNIFICANCE:\")\n    print(f\"   P-Value:                      {metrics['p_value']:.6f}\")\n    print(f\"   Baseline Success Rate:        {metrics['baseline_success_rate']*100:.2f}%\")\n    print(f\"   Sequence Success Rate:        {metrics['exceedance_ratio']*100:.2f}%\")\n    \n    if metrics['p_value'] < 0.05:\n        print(f\"   ✓ Statistically significant (p < 0.05)\")\n    else:\n        print(f\"   ✗ Not statistically significant (p >= 0.05)\")\n    \n    print(f\"\\n7. ADDITIONAL METRICS:\")\n    print(f\"   Baseline Median:              ${metrics['baseline_median']:.2f}\")\n    print(f\"   Sequence Median:              ${metrics['sequence_median']:.2f}\")\n    print(f\"   Resolution Week Used:         Week {optimal_week}\")\n    print(\"=\"*80)\n    \n    # Step 8: Generate charts\n    if generate_charts:\n        print(\"\\nGenerating verification charts...\\n\")\n        plot_baseline_distribution(ticker, baseline_data)\n        plot_bimodal_distribution(ticker, target_sequence_key, baseline_data, sequence_data)\n    \n    # Return all data\n    return {\n        'ticker': ticker,\n        'target_sequence_key': target_sequence_key,\n        'optimal_week': optimal_week,\n        'weekly_analysis': weekly_analysis_df,\n        'metrics': metrics,\n        'master_df': master_df,\n        'baseline_data': baseline_data,\n        'sequence_data': sequence_data,\n        'current_anchor_price': current_anchor_price\n    }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Verification Runs\n",
    "\n",
    "Test the framework against known analyses to verify accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification 1: KDP (Keurig Dr Pepper)\n",
    "\n",
    "The pattern will be automatically detected from the most recent 10 weeks.\n",
    "\n",
    "**Historical Expected Results (as of original analysis):**\n",
    "- Baseline clustering: ~$27.22\n",
    "- Sequence clustering: ~$29\n",
    "- Pattern: 3-7-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:21:49.825625800Z",
     "start_time": "2025-11-03T00:21:42.962027600Z"
    }
   },
   "outputs": [],
   "source": "# Run analysis for KDP\n# Pattern will be auto-detected from the most recent 10 weeks\nkdp_results = run_enomoto_analysis(\n    ticker='KDP',\n    target_resolution_week=4,  # Use 4-week profile\n    baseline_lookback_weeks=104, # Use 2-year baseline\n    generate_charts=True\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification 2: TXN (Texas Instruments)\n",
    "\n",
    "The pattern will be automatically detected from the most recent 10 weeks.\n",
    "\n",
    "**Historical Expected Results (as of original analysis):**\n",
    "- Sequence-specific clustering: ~$167\n",
    "- Recent anchor price: ~$161.46\n",
    "- Pattern: 3-7-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:22:41.783536300Z",
     "start_time": "2025-11-03T00:22:39.429199100Z"
    }
   },
   "outputs": [],
   "source": "# Run analysis for TXN\n# Pattern will be auto-detected from the most recent 10 weeks\ntxn_results = run_enomoto_analysis(\n    ticker='TXN',\n    target_resolution_week=9,  # Use 9-week profile\n    baseline_lookback_weeks=104, # Use 2-year baseline\n    generate_charts=True\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification 3: CVNA (Carvana)\n",
    "\n",
    "The pattern will be automatically detected from the most recent 10 weeks.\n",
    "\n",
    "**Historical Expected Results (as of original analysis):**\n",
    "- Baseline clustering: ~$319\n",
    "- Sequence clustering: ~$363\n",
    "- Pattern: 6-4-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:23:40.918363400Z",
     "start_time": "2025-11-03T00:23:39.156784900Z"
    }
   },
   "outputs": [],
   "source": "# Run analysis for CVNA\n# Pattern will be auto-detected from the most recent 10 weeks\ncvna_results = run_enomoto_analysis(\n    ticker='CVNA',\n    target_resolution_week=10, # Use 10-week profile\n    baseline_lookback_weeks=104, # Use 2-year baseline\n    generate_charts=True\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Summary Comparison\n",
    "\n",
    "Compare results across all three verification runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:24:21.193632200Z",
     "start_time": "2025-11-03T00:24:21.176964800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "VERIFICATION SUMMARY\n",
      "========================================================================================================================\n",
      "Ticker Baseline Clustering Sequence Clustering Positive Delta Pattern Rarity Exceedance Ratio  P-Value Significant\n",
      "   KDP              $31.67              $31.22         -1.42%          5.03%           11.76% 0.936422          No\n",
      "   TXN             $155.02             $161.67          4.29%          7.99%            3.70% 0.431664          No\n",
      "  CVNA              $47.09              $64.88         37.78%          4.73%           18.75% 0.213282          No\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create summary comparison table\n",
    "if kdp_results and txn_results and cvna_results:\n",
    "    summary_data = []\n",
    "    \n",
    "    for result in [kdp_results, txn_results, cvna_results]:\n",
    "        if result:\n",
    "            m = result['metrics']\n",
    "            summary_data.append({\n",
    "                'Ticker': result['ticker'],\n",
    "                'Baseline Clustering': f\"${m['baseline_clustering']:.2f}\",\n",
    "                'Sequence Clustering': f\"${m['sequence_clustering']:.2f}\",\n",
    "                'Positive Delta': f\"{m['positive_delta_pct']:.2f}%\",\n",
    "                'Pattern Rarity': f\"{m['pattern_rarity_pct']:.2f}%\",\n",
    "                'Exceedance Ratio': f\"{m['exceedance_ratio_pct']:.2f}%\",\n",
    "                'P-Value': f\"{m['p_value']:.6f}\",\n",
    "                'Significant': 'Yes' if m['p_value'] < 0.05 else 'No'\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"VERIFICATION SUMMARY\")\n",
    "    print(\"=\"*120)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Custom Analysis\n",
    "\n",
    "Use this cell to run your own custom analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze a custom ticker\n",
    "# Pattern will be automatically detected from the most recent 10 weeks\n",
    "# Uncomment and modify as needed\n",
    "\n",
    "# custom_results = run_enomoto_analysis(\n",
    "#     ticker='AAPL',\n",
    "#     target_resolution_week=8,\n",
    "#     generate_charts=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook implements the complete Enomoto quantitative analysis framework for identifying statistically mispriced options opportunities.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Pattern Recognition**: The \"Morse Code\" system identifies specific 10-week price patterns\n",
    "2. **Automatic Detection**: The current pattern is automatically calculated from the most recent 10 weeks\n",
    "3. **Statistical Analysis**: Compares baseline vs. sequence-specific distributions\n",
    "4. **Positive Delta**: Measures the gap in price clustering between distributions\n",
    "5. **Significance Testing**: Binomial tests validate statistical significance\n",
    "6. **Visualization**: KDE and GMM charts provide visual verification\n",
    "\n",
    "### Usage:\n",
    "\n",
    "To analyze any ticker:\n",
    "```python\n",
    "results = run_enomoto_analysis(\n",
    "    ticker='YOUR_TICKER',\n",
    "    target_resolution_week=8\n",
    ")\n",
    "```\n",
    "\n",
    "The analysis will automatically:\n",
    "- Download historical weekly data\n",
    "- Identify the current 10-week pattern\n",
    "- Find historical matches to that pattern\n",
    "- Calculate statistical metrics and probabilities\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "- All analysis uses weekly data from January 1, 2019 onwards\n",
    "- The current pattern is auto-detected from the most recent 10 weeks\n",
    "- Resolution prices use the `Low` for conservative bull call spread planning\n",
    "- P-values < 0.05 indicate statistically significant patterns\n",
    "- Higher exceedance ratios suggest better options opportunities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}