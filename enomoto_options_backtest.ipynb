{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enomoto Options Strategy Backtesting Framework\n",
    "\n",
    "This notebook implements the quantitative analysis framework of financial analyst Josh Enomoto.\n",
    "\n",
    "## Framework Overview\n",
    "\n",
    "The goal is to identify statistically mispriced options by finding a \"positive delta in price density dynamics\" - the gap between:\n",
    "- **Baseline**: Normal price behavior across all historical data\n",
    "- **Sequence-Specific**: Price behavior after a specific 10-week pattern\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Morse Code Pattern (X-Y-Z)**:\n",
    "   - X = Up weeks (Close > Open)\n",
    "   - Y = Down weeks (Close <= Open)\n",
    "   - Z = Trajectory ('D' if downward, 'U' if upward)\n",
    "\n",
    "2. **Data Period**: Weekly data from January 1, 2019 onwards\n",
    "\n",
    "3. **Price Points**:\n",
    "   - Anchor Price: Open of the first week in the 10-week window\n",
    "   - Resolution Price: Low price at a future week (e.g., 8 weeks out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and numerical libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Data acquisition\n",
    "import yfinance as yf\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import gaussian_kde, binomtest\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Acquisition Function\n",
    "\n",
    "Download weekly historical data from January 1, 2019 to present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_weekly_data(ticker, start_date='2019-01-01'):\n",
    "    \"\"\"\n",
    "    Download weekly historical data for a given ticker.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ticker : str\n",
    "        Stock ticker symbol\n",
    "    start_date : str\n",
    "        Start date for historical data (default: '2019-01-01')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with weekly OHLC data\n",
    "    \"\"\"\n",
    "    print(f\"\\nDownloading weekly data for {ticker} from {start_date}...\")\n",
    "    \n",
    "    # Download data with weekly interval\n",
    "    data = yf.download(ticker, start=start_date, interval='1wk', progress=False)\n",
    "    \n",
    "    # Reset index to make Date a column\n",
    "    data = data.reset_index()\n",
    "    \n",
    "    # Ensure we have the necessary columns\n",
    "    required_cols = ['Date', 'Open', 'Close', 'Low', 'High']\n",
    "    for col in required_cols:\n",
    "        if col not in data.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    \n",
    "    print(f\"Downloaded {len(data)} weeks of data\")\n",
    "    print(f\"Date range: {data['Date'].min()} to {data['Date'].max()}\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering - The \"Morse Code\" Pattern\n",
    "\n",
    "Implement the pattern detection logic that identifies 10-week sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence(window):\n",
    "    \"\"\"\n",
    "    Calculate the 'Morse Code' pattern for a 10-week window.\n",
    "    \n",
    "    Pattern format: \"X-Y-Z\" where:\n",
    "    - X = number of up weeks (Close > Open)\n",
    "    - Y = number of down weeks (Close <= Open)\n",
    "    - Z = trajectory ('D' for downward, 'U' for upward)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    window : pd.DataFrame\n",
    "        10-week DataFrame window with 'Open' and 'Close' columns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Pattern string in format \"X-Y-Z\" (e.g., \"3-7-D\")\n",
    "    \"\"\"\n",
    "    # Count up weeks (Close > Open)\n",
    "    up_weeks = (window['Close'] > window['Open']).sum()\n",
    "    \n",
    "    # Count down weeks (Close <= Open)\n",
    "    down_weeks = (window['Close'] <= window['Open']).sum()\n",
    "    \n",
    "    # Determine trajectory: Compare last Close to first Open\n",
    "    # 'D' if closing price of 10th week < opening price of 1st week\n",
    "    trajectory = 'D' if window['Close'].iloc[-1] < window['Open'].iloc[0] else 'U'\n",
    "    \n",
    "    # Format as \"X-Y-Z\"\n",
    "    sequence_key = f\"{up_weeks}-{down_weeks}-{trajectory}\"\n",
    "    \n",
    "    return sequence_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Historical Pattern Analysis - Rolling Window\n",
    "\n",
    "Create a master DataFrame that analyzes all 10-week windows and their future outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_master_dataframe(data, window_size=10, future_weeks=10):\n",
    "    \"\"\"\n",
    "    Build master analysis DataFrame with all 10-week windows and their future outcomes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Historical weekly data\n",
    "    window_size : int\n",
    "        Size of the rolling window (default: 10)\n",
    "    future_weeks : int\n",
    "        Number of future weeks to track (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Master DataFrame with pattern analysis\n",
    "    \"\"\"\n",
    "    print(f\"\\nBuilding master DataFrame with {window_size}-week rolling windows...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Iterate through all possible 10-week windows\n",
    "    for i in range(len(data) - window_size - future_weeks + 1):\n",
    "        # Extract the 10-week window\n",
    "        window = data.iloc[i:i + window_size]\n",
    "        \n",
    "        # Calculate pattern sequence\n",
    "        sequence_key = get_sequence(window)\n",
    "        \n",
    "        # Get anchor price (Open of first week)\n",
    "        anchor_price = window['Open'].iloc[0]\n",
    "        \n",
    "        # Get start and end dates\n",
    "        start_date = window['Date'].iloc[0]\n",
    "        end_date = window['Date'].iloc[-1]\n",
    "        \n",
    "        # Build result row\n",
    "        row = {\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'anchor_price': anchor_price,\n",
    "            'sequence_key': sequence_key\n",
    "        }\n",
    "        \n",
    "        # Add future resolution prices (Low prices for next N weeks)\n",
    "        for week_offset in range(1, future_weeks + 1):\n",
    "            future_idx = i + window_size + week_offset - 1\n",
    "            if future_idx < len(data):\n",
    "                row[f'future_week_{week_offset}_low'] = data['Low'].iloc[future_idx]\n",
    "            else:\n",
    "                row[f'future_week_{week_offset}_low'] = np.nan\n",
    "        \n",
    "        results.append(row)\n",
    "    \n",
    "    master_df = pd.DataFrame(results)\n",
    "    print(f\"Created master DataFrame with {len(master_df)} windows\")\n",
    "    print(f\"Found {master_df['sequence_key'].nunique()} unique patterns\")\n",
    "    \n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Distribution Analysis and Metric Calculation\n",
    "\n",
    "Calculate key statistical metrics including clustering, delta, and p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clustering_price(data, bandwidth='scott'):\n",
    "    \"\"\"\n",
    "    Calculate the clustering price (mode/peak) of a distribution using KDE.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.Series or np.array\n",
    "        Price data\n",
    "    bandwidth : str or float\n",
    "        KDE bandwidth method (default: 'scott')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        The price at the peak of the KDE distribution\n",
    "    \"\"\"\n",
    "    # Remove NaN values\n",
    "    clean_data = data.dropna()\n",
    "    \n",
    "    if len(clean_data) < 2:\n",
    "        return np.nan\n",
    "    \n",
    "    # Fit KDE\n",
    "    kde = gaussian_kde(clean_data, bw_method=bandwidth)\n",
    "    \n",
    "    # Create evaluation grid\n",
    "    x_grid = np.linspace(clean_data.min(), clean_data.max(), 1000)\n",
    "    kde_values = kde(x_grid)\n",
    "    \n",
    "    # Find peak (mode)\n",
    "    peak_idx = np.argmax(kde_values)\n",
    "    clustering_price = x_grid[peak_idx]\n",
    "    \n",
    "    return clustering_price\n",
    "\n",
    "\n",
    "def calculate_metrics(master_df, target_sequence_key, resolution_price_col, current_anchor_price):\n",
    "    \"\"\"\n",
    "    Calculate all core Enomoto metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    master_df : pd.DataFrame\n",
    "        Master analysis DataFrame\n",
    "    target_sequence_key : str\n",
    "        Target pattern (e.g., '3-7-D')\n",
    "    resolution_price_col : str\n",
    "        Column name for resolution price (e.g., 'future_week_8_low')\n",
    "    current_anchor_price : float\n",
    "        Current anchor price for exceedance calculation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with all calculated metrics\n",
    "    \"\"\"\n",
    "    # Extract baseline and sequence-specific data\n",
    "    baseline_data = master_df[resolution_price_col].dropna()\n",
    "    sequence_mask = master_df['sequence_key'] == target_sequence_key\n",
    "    sequence_data = master_df.loc[sequence_mask, resolution_price_col].dropna()\n",
    "    \n",
    "    if len(sequence_data) == 0:\n",
    "        print(f\"WARNING: No data found for sequence '{target_sequence_key}'\")\n",
    "        return None\n",
    "    \n",
    "    # 1. Calculate clustering prices (mode of KDE)\n",
    "    baseline_clustering = calculate_clustering_price(baseline_data)\n",
    "    sequence_clustering = calculate_clustering_price(sequence_data)\n",
    "    \n",
    "    # 2. Positive Delta\n",
    "    positive_delta = (sequence_clustering - baseline_clustering) / baseline_clustering\n",
    "    \n",
    "    # 3. Pattern Rarity\n",
    "    pattern_rarity = len(sequence_data) / len(baseline_data)\n",
    "    \n",
    "    # 4. Exceedance Ratio (sequence-specific)\n",
    "    exceedance_ratio = (sequence_data > current_anchor_price).mean()\n",
    "    \n",
    "    # 5. Terminal Median\n",
    "    terminal_median = sequence_data.median()\n",
    "    \n",
    "    # 6. P-Value (Binomial Test)\n",
    "    baseline_success_rate = (baseline_data > current_anchor_price).mean()\n",
    "    sequence_success_count = (sequence_data > current_anchor_price).sum()\n",
    "    n_trials = len(sequence_data)\n",
    "    \n",
    "    # Perform binomial test\n",
    "    binom_result = binomtest(sequence_success_count, n=n_trials, p=baseline_success_rate, alternative='greater')\n",
    "    p_value = binom_result.pvalue\n",
    "    \n",
    "    # Compile metrics\n",
    "    metrics = {\n",
    "        'baseline_clustering': baseline_clustering,\n",
    "        'sequence_clustering': sequence_clustering,\n",
    "        'positive_delta': positive_delta,\n",
    "        'positive_delta_pct': positive_delta * 100,\n",
    "        'pattern_rarity': pattern_rarity,\n",
    "        'pattern_rarity_pct': pattern_rarity * 100,\n",
    "        'exceedance_ratio': exceedance_ratio,\n",
    "        'exceedance_ratio_pct': exceedance_ratio * 100,\n",
    "        'terminal_median': terminal_median,\n",
    "        'p_value': p_value,\n",
    "        'baseline_success_rate': baseline_success_rate,\n",
    "        'sequence_success_count': sequence_success_count,\n",
    "        'n_trials': n_trials,\n",
    "        'baseline_median': baseline_data.median(),\n",
    "        'sequence_median': sequence_data.median(),\n",
    "        'baseline_count': len(baseline_data),\n",
    "        'sequence_count': len(sequence_data)\n",
    "    }\n",
    "    \n",
    "    return metrics, baseline_data, sequence_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Chart Generation\n",
    "\n",
    "Create verification charts: Baseline distribution and bimodal comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_baseline_distribution(ticker, baseline_data):\n",
    "    \"\"\"\n",
    "    Plot Chart 1: Simple baseline distribution with median line.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ticker : str\n",
    "        Stock ticker\n",
    "    baseline_data : pd.Series\n",
    "        Baseline price data\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot KDE\n",
    "    sns.kdeplot(data=baseline_data, ax=ax, color='steelblue', linewidth=2, fill=True, alpha=0.3)\n",
    "    \n",
    "    # Add median line\n",
    "    median_val = baseline_data.median()\n",
    "    ax.axvline(median_val, color='red', linestyle='--', linewidth=2, label=f'Median: ${median_val:.2f}')\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel('Price ($)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Density', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{ticker} - Baseline Distribution (All Outcomes)', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_bimodal_distribution(ticker, target_sequence_key, baseline_data, sequence_data):\n",
    "    \"\"\"\n",
    "    Plot Chart 2: Bimodal/juxtaposed distribution with GMM overlay.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ticker : str\n",
    "        Stock ticker\n",
    "    target_sequence_key : str\n",
    "        Target pattern\n",
    "    baseline_data : pd.Series\n",
    "        Baseline price data\n",
    "    sequence_data : pd.Series\n",
    "        Sequence-specific price data\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "    # Layer 1: Baseline Distribution KDE\n",
    "    sns.kdeplot(data=baseline_data, ax=ax, color='green', linewidth=2, \n",
    "                fill=True, alpha=0.3, label='Baseline (All Outcomes)')\n",
    "    \n",
    "    # Layer 2: Sequence-Specific Distribution KDE\n",
    "    sns.kdeplot(data=sequence_data, ax=ax, color='blue', linewidth=2.5, \n",
    "                fill=False, alpha=0.8, label=f'Sequence-Specific ({target_sequence_key})')\n",
    "    \n",
    "    # Layer 3: Gaussian Mixture Model (Bimodal)\n",
    "    if len(sequence_data) >= 2:\n",
    "        try:\n",
    "            # Fit GMM with 2 components\n",
    "            gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "            gmm.fit(sequence_data.values.reshape(-1, 1))\n",
    "            \n",
    "            # Generate PDF for GMM\n",
    "            x_range = np.linspace(sequence_data.min(), sequence_data.max(), 1000)\n",
    "            \n",
    "            # Calculate GMM PDF\n",
    "            logprob = gmm.score_samples(x_range.reshape(-1, 1))\n",
    "            pdf = np.exp(logprob)\n",
    "            \n",
    "            # Plot GMM\n",
    "            ax.plot(x_range, pdf, color='red', linestyle='--', linewidth=2, \n",
    "                   label='Bimodal (GMM)', alpha=0.8)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not fit GMM - {e}\")\n",
    "    \n",
    "    # Add median lines\n",
    "    baseline_median = baseline_data.median()\n",
    "    sequence_median = sequence_data.median()\n",
    "    \n",
    "    ax.axvline(baseline_median, color='green', linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "    ax.axvline(sequence_median, color='blue', linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel('Price ($)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Density', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{ticker} - Bimodal Distribution (Sequence: {target_sequence_key})', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10, loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Main Analysis Function\n",
    "\n",
    "Integrate all components into a single analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_enomoto_analysis(ticker, target_sequence_key, target_resolution_week=8, \n",
    "                         start_date='2019-01-01', generate_charts=True):\n",
    "    \"\"\"\n",
    "    Run complete Enomoto analysis for a given ticker and pattern.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ticker : str\n",
    "        Stock ticker symbol\n",
    "    target_sequence_key : str\n",
    "        Target pattern to analyze (e.g., '3-7-D')\n",
    "    target_resolution_week : int\n",
    "        Future week for resolution price (default: 8)\n",
    "    start_date : str\n",
    "        Start date for historical data (default: '2019-01-01')\n",
    "    generate_charts : bool\n",
    "        Whether to generate verification charts (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with all metrics and data\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ENOMOTO ANALYSIS: {ticker}\")\n",
    "    print(f\"Target Sequence: {target_sequence_key}\")\n",
    "    print(f\"Resolution Week: {target_resolution_week}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Download data\n",
    "    data = download_weekly_data(ticker, start_date)\n",
    "    \n",
    "    # Step 2: Build master DataFrame\n",
    "    master_df = build_master_dataframe(data)\n",
    "    \n",
    "    # Step 3: Define resolution column\n",
    "    resolution_price_col = f'future_week_{target_resolution_week}_low'\n",
    "    \n",
    "    # Step 4: Get current anchor price (most recent 10-week window)\n",
    "    current_anchor_price = master_df['anchor_price'].iloc[-1]\n",
    "    print(f\"\\nCurrent anchor price: ${current_anchor_price:.2f}\")\n",
    "    \n",
    "    # Step 5: Calculate metrics\n",
    "    print(\"\\nCalculating metrics...\")\n",
    "    result = calculate_metrics(master_df, target_sequence_key, resolution_price_col, current_anchor_price)\n",
    "    \n",
    "    if result is None:\n",
    "        print(\"Analysis failed - no data for target sequence\")\n",
    "        return None\n",
    "    \n",
    "    metrics, baseline_data, sequence_data = result\n",
    "    \n",
    "    # Step 6: Print metrics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n1. PRICE CLUSTERING:\")\n",
    "    print(f\"   Baseline Clustering:          ${metrics['baseline_clustering']:.2f}\")\n",
    "    print(f\"   Sequence Clustering:          ${metrics['sequence_clustering']:.2f}\")\n",
    "    print(f\"\\n2. POSITIVE DELTA:              {metrics['positive_delta_pct']:.2f}%\")\n",
    "    print(f\"\\n3. PATTERN RARITY:              {metrics['pattern_rarity_pct']:.2f}% ({metrics['sequence_count']}/{metrics['baseline_count']} windows)\")\n",
    "    print(f\"\\n4. EXCEEDANCE RATIO:            {metrics['exceedance_ratio_pct']:.2f}%\")\n",
    "    print(f\"   (Probability resolution > anchor)\")\n",
    "    print(f\"\\n5. TERMINAL MEDIAN:             ${metrics['terminal_median']:.2f}\")\n",
    "    print(f\"\\n6. STATISTICAL SIGNIFICANCE:\")\n",
    "    print(f\"   P-Value:                      {metrics['p_value']:.6f}\")\n",
    "    print(f\"   Baseline Success Rate:        {metrics['baseline_success_rate']*100:.2f}%\")\n",
    "    print(f\"   Sequence Success Rate:        {metrics['exceedance_ratio']*100:.2f}%\")\n",
    "    \n",
    "    if metrics['p_value'] < 0.05:\n",
    "        print(f\"   ✓ Statistically significant (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"   ✗ Not statistically significant (p >= 0.05)\")\n",
    "    \n",
    "    print(f\"\\n7. ADDITIONAL METRICS:\")\n",
    "    print(f\"   Baseline Median:              ${metrics['baseline_median']:.2f}\")\n",
    "    print(f\"   Sequence Median:              ${metrics['sequence_median']:.2f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 7: Generate charts\n",
    "    if generate_charts:\n",
    "        print(\"\\nGenerating verification charts...\\n\")\n",
    "        plot_baseline_distribution(ticker, baseline_data)\n",
    "        plot_bimodal_distribution(ticker, target_sequence_key, baseline_data, sequence_data)\n",
    "    \n",
    "    # Return all data\n",
    "    return {\n",
    "        'ticker': ticker,\n",
    "        'metrics': metrics,\n",
    "        'master_df': master_df,\n",
    "        'baseline_data': baseline_data,\n",
    "        'sequence_data': sequence_data,\n",
    "        'current_anchor_price': current_anchor_price\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Verification Runs\n",
    "\n",
    "Test the framework against known analyses to verify accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification 1: KDP (Keurig Dr Pepper)\n",
    "\n",
    "**Expected Results:**\n",
    "- Baseline clustering: ~$27.22\n",
    "- Sequence clustering: ~$29\n",
    "- Pattern: 3-7-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis for KDP\n",
    "kdp_results = run_enomoto_analysis(\n",
    "    ticker='KDP',\n",
    "    target_sequence_key='3-7-D',\n",
    "    target_resolution_week=8,\n",
    "    generate_charts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification 2: TXN (Texas Instruments)\n",
    "\n",
    "**Expected Results:**\n",
    "- Sequence-specific clustering: ~$167\n",
    "- Recent anchor price: ~$161.46\n",
    "- Pattern: 3-7-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis for TXN\n",
    "txn_results = run_enomoto_analysis(\n",
    "    ticker='TXN',\n",
    "    target_sequence_key='3-7-D',\n",
    "    target_resolution_week=8,\n",
    "    generate_charts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification 3: CVNA (Carvana)\n",
    "\n",
    "**Expected Results:**\n",
    "- Baseline clustering: ~$319\n",
    "- Sequence clustering: ~$363\n",
    "- Pattern: 6-4-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis for CVNA\n",
    "cvna_results = run_enomoto_analysis(\n",
    "    ticker='CVNA',\n",
    "    target_sequence_key='6-4-D',\n",
    "    target_resolution_week=8,\n",
    "    generate_charts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Summary Comparison\n",
    "\n",
    "Compare results across all three verification runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary comparison table\n",
    "if kdp_results and txn_results and cvna_results:\n",
    "    summary_data = []\n",
    "    \n",
    "    for result in [kdp_results, txn_results, cvna_results]:\n",
    "        if result:\n",
    "            m = result['metrics']\n",
    "            summary_data.append({\n",
    "                'Ticker': result['ticker'],\n",
    "                'Baseline Clustering': f\"${m['baseline_clustering']:.2f}\",\n",
    "                'Sequence Clustering': f\"${m['sequence_clustering']:.2f}\",\n",
    "                'Positive Delta': f\"{m['positive_delta_pct']:.2f}%\",\n",
    "                'Pattern Rarity': f\"{m['pattern_rarity_pct']:.2f}%\",\n",
    "                'Exceedance Ratio': f\"{m['exceedance_ratio_pct']:.2f}%\",\n",
    "                'P-Value': f\"{m['p_value']:.6f}\",\n",
    "                'Significant': 'Yes' if m['p_value'] < 0.05 else 'No'\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"VERIFICATION SUMMARY\")\n",
    "    print(\"=\"*120)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Custom Analysis\n",
    "\n",
    "Use this cell to run your own custom analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze a custom ticker and pattern\n",
    "# Uncomment and modify as needed\n",
    "\n",
    "# custom_results = run_enomoto_analysis(\n",
    "#     ticker='AAPL',\n",
    "#     target_sequence_key='5-5-U',\n",
    "#     target_resolution_week=8,\n",
    "#     generate_charts=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook implements the complete Enomoto quantitative analysis framework for identifying statistically mispriced options opportunities.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Pattern Recognition**: The \"Morse Code\" system identifies specific 10-week price patterns\n",
    "2. **Statistical Analysis**: Compares baseline vs. sequence-specific distributions\n",
    "3. **Positive Delta**: Measures the gap in price clustering between distributions\n",
    "4. **Significance Testing**: Binomial tests validate statistical significance\n",
    "5. **Visualization**: KDE and GMM charts provide visual verification\n",
    "\n",
    "### Usage:\n",
    "\n",
    "To analyze any ticker:\n",
    "```python\n",
    "results = run_enomoto_analysis(\n",
    "    ticker='YOUR_TICKER',\n",
    "    target_sequence_key='X-Y-Z',\n",
    "    target_resolution_week=8\n",
    ")\n",
    "```\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "- All analysis uses weekly data from January 1, 2019 onwards\n",
    "- Resolution prices use the `Low` for conservative bull call spread planning\n",
    "- P-values < 0.05 indicate statistically significant patterns\n",
    "- Higher exceedance ratios suggest better options opportunities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
