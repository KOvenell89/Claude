{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import linregress\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "TICKER = 'ADP'\n",
    "\n",
    "print(f\"Downloading daily data for {TICKER} from January 1, 2019...\")\n",
    "data_daily = yf.download(TICKER, start='2019-01-01', interval='1d', progress=False)\n",
    "\n",
    "if isinstance(data_daily.columns, pd.MultiIndex):\n",
    "    data_daily.columns = data_daily.columns.droplevel(1)\n",
    "\n",
    "# Resample to weekly, ending on Fridays\n",
    "data = data_daily.resample('W-FRI').agg({\n",
    "    'Open': 'first',\n",
    "    'High': 'max',\n",
    "    'Low': 'min',\n",
    "    'Close': 'last',\n",
    "    'Volume': 'sum'\n",
    "}).dropna()\n",
    "\n",
    "print(f\"Total weeks: {len(data)}\")\n",
    "print(f\"Range: {data.index[0].date()} to {data.index[-1].date()}\")\n",
    "print(f\"\\nLast few rows:\")\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 1: Analyze most recent 10 weeks to generate X-Y-D/U pattern notation.\n",
    "\"\"\"\n",
    "\n",
    "current_window = data.tail(10).copy()\n",
    "\n",
    "# Count up/down weeks (Close vs prior Close)\n",
    "current_window['Up'] = (current_window['Close'] > current_window['Close'].shift(1)).astype(int)\n",
    "up_weeks = current_window['Up'].sum()\n",
    "down_weeks = 10 - up_weeks\n",
    "\n",
    "# Calculate trajectory\n",
    "close_prices = current_window['Close'].values.flatten()\n",
    "weeks_index = np.arange(len(close_prices))\n",
    "slope, intercept, r_value, p_value, std_err = linregress(weeks_index, close_prices)\n",
    "trajectory = 'U' if slope > 0 else 'D'\n",
    "\n",
    "# Entry price = closing price at END of pattern\n",
    "entry_price = float(current_window.iloc[-1]['Close'].iloc[0] if hasattr(current_window.iloc[-1]['Close'], 'iloc') else current_window.iloc[-1]['Close'])\n",
    "\n",
    "current_pattern = f\"{up_weeks}-{down_weeks}-{trajectory}\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: CURRENT 10-WEEK SEQUENCE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Pattern: {current_pattern}\")\n",
    "print(f\"Up Weeks: {up_weeks}\")\n",
    "print(f\"Down Weeks: {down_weeks}\")\n",
    "print(f\"Trajectory: {trajectory} (slope: {slope:.4f})\")\n",
    "print(f\"Entry Price: ${entry_price:.2f}\")\n",
    "print(f\"Period: {current_window.index[0].date()} to {current_window.index[-1].date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nSTEP 2: Scan historical data for matching pattern instances.\n\"\"\"\n\ndef calculate_pattern(window_df):\n    if len(window_df) < 10:\n        return None\n    up = (window_df['Close'] > window_df['Close'].shift(1)).sum()\n    down = 10 - up\n    closes = window_df['Close'].values.flatten()\n    idx = np.arange(len(closes))\n    slope, _, _, _, _ = linregress(idx, closes)\n    traj = 'U' if slope > 0 else 'D'\n    return f\"{up}-{down}-{traj}\"\n\npatterns_list = []\n\nfor i in range(len(data) - 9):\n    window = data.iloc[i:i+10]\n    pattern = calculate_pattern(window)\n    if pattern:\n        pattern_end_price = float(window.iloc[-1]['Close'].iloc[0] if hasattr(window.iloc[-1]['Close'], 'iloc') else window.iloc[-1]['Close'])\n        patterns_list.append({\n            'pattern': pattern,\n            'start_date': window.index[0],\n            'end_date': window.index[-1],\n            'pattern_end_price': pattern_end_price,\n            'window_idx': i\n        })\n\npatterns_df = pd.DataFrame(patterns_list)\nmatches = patterns_df[patterns_df['pattern'] == current_pattern]\ntotal_patterns = len(patterns_df)\nmatch_count = len(matches)\nfrequency = (match_count / total_patterns) * 100 if total_patterns > 0 else 0\n\n# ENOMOTO METHODOLOGY: Use only last 10 matches (L10) for pattern-specific analysis\nmatches_l10 = matches.tail(10)\nl10_count = len(matches_l10)\n\nprint(\"=\" * 60)\nprint(\"STEP 2: HISTORICAL PATTERN OCCURRENCES\")\nprint(\"=\" * 60)\nprint(f\"Current Pattern: {current_pattern}\")\nprint(f\"Total Historical Windows: {total_patterns}\")\nprint(f\"Total Matching Patterns: {match_count}\")\nprint(f\"L10 Matches (Last 10): {l10_count}\")\nprint(f\"Frequency: {frequency:.2f}%\")\nprint(f\"Rarity: {'Common' if frequency > 5 else 'Moderate' if frequency > 2 else 'Rare'}\")\n\nif match_count > 0:\n    print(f\"\\nSample matches (showing all if ‚â§10, or last 10):\")\n    print(matches_l10[['start_date', 'end_date', 'pattern_end_price']])\nelse:\n    print(f\"\\n‚ö†Ô∏è  No matches found - analysis will use baseline only\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nSTEP 3 & 4 (ENOMOTO BARCHART METHODOLOGY): \nCalculate baseline and L10 pattern-specific distributions, then create bimodal GMM on combined data.\n\nKEY ENOMOTO METHODOLOGY:\n1. Use ALL weeks 1-10 instead of just week 10\n2. Use ACTUAL historical prices instead of projecting onto entry_price\n3. Use only LAST 10 MATCHES (L10) for pattern-specific analysis\n4. Create BIMODAL GMM on COMBINED baseline + L10 data\n5. Barchart-style visualization with blue baseline, green L10, and red GMM curve\n\"\"\"\n\nFORWARD_WEEKS = 10\n\ndef get_price_clustering_gmm(prices_array, n_components_range=(1, 4)):\n    \"\"\"\n    Uses GMM with BIC comparison to find optimal clustering.\n    Returns the dominant cluster price (highest weight component).\n    \"\"\"\n    if len(prices_array) < 20:\n        if len(prices_array) > 0:\n            return np.median(prices_array), 1, None\n        else:\n            return np.nan, 0, None\n    \n    X = prices_array.reshape(-1, 1)\n    \n    best_gmm = None\n    best_bic = np.inf\n    \n    for n_components in range(n_components_range[0], n_components_range[1]):\n        gmm = GaussianMixture(n_components=n_components, random_state=42)\n        gmm.fit(X)\n        bic = gmm.bic(X)\n        if bic < best_bic:\n            best_bic = bic\n            best_gmm = gmm\n    \n    # Find dominant cluster (highest weight)\n    dominant_idx = np.argmax(best_gmm.weights_)\n    cluster_price = best_gmm.means_[dominant_idx][0]\n    \n    return cluster_price, best_gmm.n_components, best_gmm\n\n# === BASELINE: Collect ALL future prices from ALL patterns ===\nbaseline_future_prices = []\n\nfor idx, row in patterns_df.iterrows():\n    window_end_idx = row['window_idx'] + 9  # Last week of pattern\n    \n    # Collect ALL prices from weeks 1-10 ahead (not just week 10)\n    for week_offset in range(1, FORWARD_WEEKS + 1):\n        future_idx = window_end_idx + week_offset\n        if future_idx < len(data):\n            future_price = data.iloc[future_idx]['Close']\n            baseline_future_prices.append(float(future_price.iloc[0] if hasattr(future_price, 'iloc') else future_price))\n\nbaseline_prices_array = np.array(baseline_future_prices)\nbaseline_median = np.median(baseline_prices_array)\n\nprint(\"=\" * 60)\nprint(\"STEP 3: BASELINE EXPECTATIONS (GLOBAL)\")\nprint(\"=\" * 60)\nprint(f\"Sample Size: {len(baseline_prices_array)} price points\")\nprint(f\"  ({len(patterns_df)} patterns √ó {FORWARD_WEEKS} weeks)\")\nprint(f\"Forward Period: {FORWARD_WEEKS} weeks\")\nprint(f\"\\nBaseline Statistics (using ACTUAL historical prices):\")\nprint(f\"  Mean: ${baseline_prices_array.mean():.2f}\")\nprint(f\"  Global Median: ${baseline_median:.2f}\")\n\n# === PATTERN-SPECIFIC L10: Collect ALL future prices from LAST 10 MATCHING patterns ===\npattern_l10_future_prices = []\n\nfor idx, row in matches_l10.iterrows():\n    window_end_idx = row['window_idx'] + 9\n    \n    # Collect ALL prices from weeks 1-10 ahead\n    for week_offset in range(1, FORWARD_WEEKS + 1):\n        future_idx = window_end_idx + week_offset\n        if future_idx < len(data):\n            future_price = data.iloc[future_idx]['Close']\n            pattern_l10_future_prices.append(float(future_price.iloc[0] if hasattr(future_price, 'iloc') else future_price))\n\nif len(pattern_l10_future_prices) > 0:\n    pattern_l10_prices_array = np.array(pattern_l10_future_prices)\n    pattern_l10_median = np.median(pattern_l10_prices_array)\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"STEP 4: L10 PATTERN-SPECIFIC DISTRIBUTION\")\n    print(\"=\" * 60)\n    print(f\"Pattern: {current_pattern}\")\n    print(f\"Sample Size: {len(pattern_l10_prices_array)} price points\")\n    print(f\"  ({l10_count} patterns √ó {FORWARD_WEEKS} weeks)\")\n    print(f\"\\nL10 Pattern Statistics (using ACTUAL historical prices):\")\n    print(f\"  Mean: ${pattern_l10_prices_array.mean():.2f}\")\n    print(f\"  L10 Median: ${pattern_l10_median:.2f}\")\n    \n    # === ENOMOTO BIMODAL GMM: Fit 2-component GMM on COMBINED data ===\n    combined_prices = np.concatenate([baseline_prices_array, pattern_l10_prices_array])\n    bimodal_gmm = GaussianMixture(n_components=2, random_state=42)\n    bimodal_gmm.fit(combined_prices.reshape(-1, 1))\n    \n    # Extract the two means\n    gmm_mean_1 = bimodal_gmm.means_[0][0]\n    gmm_mean_2 = bimodal_gmm.means_[1][0]\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"BIMODAL GMM ON COMBINED DATA\")\n    print(\"=\" * 60)\n    print(f\"Combined Sample: {len(combined_prices)} price points\")\n    print(f\"  (Baseline: {len(baseline_prices_array)} + L10: {len(pattern_l10_prices_array)})\")\n    print(f\"GMM Component 1 Mean: ${gmm_mean_1:.2f}\")\n    print(f\"GMM Component 2 Mean: ${gmm_mean_2:.2f}\")\n    \n    # === BARCHART-STYLE VISUALIZATION ===\n    plt.figure(figsize=(14, 7))\n    \n    # Plot baseline KDE as blue filled area\n    sns.kdeplot(data=baseline_prices_array, fill=True, label='All outcomes', \n                alpha=0.3, color='blue')\n    \n    # Plot L10 pattern KDE as green filled area\n    pattern_label = f'L10 ({current_pattern})'\n    sns.kdeplot(data=pattern_l10_prices_array, fill=True, label=pattern_label,\n                alpha=0.5, color='green')\n    \n    # Plot bimodal GMM as RED CURVE\n    x_min = min(combined_prices.min(), baseline_prices_array.min(), pattern_l10_prices_array.min())\n    x_max = max(combined_prices.max(), baseline_prices_array.max(), pattern_l10_prices_array.max())\n    x_range = np.linspace(x_min, x_max, 1000).reshape(-1, 1)\n    \n    # Calculate GMM probabilities using score_samples\n    log_prob = bimodal_gmm.score_samples(x_range)\n    prob = np.exp(log_prob)\n    \n    # Plot as smooth red curve\n    plt.plot(x_range, prob, color='red', linewidth=2, label='Bimodal Fit (GMM)')\n    \n    # Vertical lines for medians and entry price\n    plt.axvline(baseline_median, color='blue', linestyle='--', linewidth=1.5,\n                label=f'Global Median: ${baseline_median:.2f}')\n    plt.axvline(pattern_l10_median, color='green', linestyle='--', linewidth=1.5,\n                label=f'L10 Median: ${pattern_l10_median:.2f}')\n    plt.axvline(entry_price, color='black', linestyle='-', linewidth=2,\n                label=f'Entry Price: ${entry_price:.2f}')\n    \n    plt.xlabel(f'Price (Weeks 1-{FORWARD_WEEKS} ahead)', fontsize=12)\n    plt.ylabel('Density', fontsize=12)\n    plt.title(f'L10 Median vs Global Median - {current_pattern}\\n' +\n              f'Baseline: {len(baseline_prices_array)} prices vs L10: {len(pattern_l10_prices_array)} prices',\n              fontsize=14, fontweight='bold')\n    plt.legend(loc='upper right', fontsize=10)\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \nelse:\n    pattern_l10_median = np.nan\n    pattern_l10_prices_array = np.array([])\n    bimodal_gmm = None\n    print(\"\\nNo L10 pattern matches found - cannot calculate pattern-specific distribution.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nSTEP 5: Calculate delta and exceedance metrics (ENOMOTO L10 METHODOLOGY).\n\"\"\"\n\nif len(pattern_l10_future_prices) > 0 and not np.isnan(pattern_l10_median):\n    # Delta based on L10 median vs global median\n    delta_pct = ((pattern_l10_median - baseline_median) / baseline_median) * 100\n    \n    exceedance_count = (pattern_l10_prices_array > entry_price).sum()\n    exceedance_ratio = (exceedance_count / len(pattern_l10_prices_array)) * 100\n    \n    terminal_median_price = pattern_l10_median\n    \n    print(\"=\" * 60)\n    print(\"STEP 5: DELTA IN PRICE DENSITY DYNAMICS (L10)\")\n    print(\"=\" * 60)\n    print(f\"Global Median: ${baseline_median:.2f}\")\n    print(f\"L10 Median: ${pattern_l10_median:.2f}\")\n    print(f\"Delta: {delta_pct:+.2f}%\")\n    print(f\"Signal: {'BULLISH - Positive delta' if delta_pct > 0 else 'BEARISH - Negative delta'}\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"STEP 6: SUPPORTING METRICS\")\n    print(\"=\" * 60)\n    print(f\"Entry Price: ${entry_price:.2f}\")\n    print(f\"L10 Exceedance Ratio: {exceedance_ratio:.1f}%\")\n    print(f\"L10 Terminal Median: ${terminal_median_price:.2f}\")\n    print(f\"L10 Sample Size: {len(pattern_l10_prices_array)} price points from {l10_count} pattern instances\")\n    \n    # Compare to baseline statistics\n    baseline_exceedance = (baseline_prices_array > entry_price).sum()\n    baseline_exceedance_ratio = (baseline_exceedance / len(baseline_prices_array)) * 100\n    print(f\"\\nComparison to Baseline:\")\n    print(f\"  Baseline Exceedance Ratio: {baseline_exceedance_ratio:.1f}%\")\n    print(f\"  L10 Exceedance Ratio: {exceedance_ratio:.1f}%\")\n    print(f\"  Difference: {exceedance_ratio - baseline_exceedance_ratio:+.1f}%\")\n    \n    METRICS = {\n        'delta_pct': delta_pct,\n        'exceedance_ratio': exceedance_ratio,\n        'pattern_l10_median': pattern_l10_median,\n        'terminal_median_price': terminal_median_price,\n        'baseline_median': baseline_median,\n        'pattern_l10_mean': pattern_l10_prices_array.mean(),\n        'baseline_mean': baseline_prices_array.mean(),\n        'l10_count': l10_count,\n        'l10_sample_size': len(pattern_l10_prices_array)\n    }\nelse:\n    METRICS = None\n    print(\"\\nNo L10 pattern matches - cannot calculate delta.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nSTEP 7: Design bull call spread based on positive delta exploitation (ENOMOTO L10 METHODOLOGY).\n\"\"\"\n\nif METRICS and METRICS['delta_pct'] > 0:\n    last_date = data.index[-1]\n    expiration_date = last_date + timedelta(weeks=FORWARD_WEEKS)\n    \n    # Strike selection: center on L10 median\n    pattern_l10_median_price = METRICS['pattern_l10_median']\n    \n    # Short strike targets the L10 median (where price likely lands)\n    short_strike = round(pattern_l10_median_price * 2) / 2  # Round to $0.50\n    \n    # Long strike 5-10 points below for spread width\n    spread_width = 10 if entry_price > 200 else 5\n    long_strike = short_strike - spread_width\n    \n    # Alternative conservative spread using baseline\n    conservative_long = round(METRICS['baseline_median'] * 2) / 2\n    conservative_short = conservative_long + spread_width\n    \n    upside_to_l10 = ((pattern_l10_median_price - entry_price) / entry_price) * 100\n    \n    print(\"=\" * 60)\n    print(f\"STEP 7: BULL CALL SPREAD RECOMMENDATION - {TICKER} (ENOMOTO L10)\")\n    print(\"=\" * 60)\n    print(f\"\\nüìä PATTERN: {current_pattern}\")\n    print(f\"   Total Historical Matches: {match_count}\")\n    print(f\"   L10 Matches Used: {l10_count}\")\n    print(f\"   Frequency: {frequency:.2f}% ({'Ultra-rare' if frequency < 3 else 'Rare' if frequency < 5 else 'Moderate'})\")\n    \n    print(f\"\\nüìà KEY METRICS (ENOMOTO L10 METHODOLOGY):\")\n    print(f\"   Positive Delta: +{METRICS['delta_pct']:.2f}%\")\n    print(f\"   Global Median: ${METRICS['baseline_median']:.2f}\")\n    print(f\"   L10 Median: ${pattern_l10_median_price:.2f}\")\n    print(f\"   L10 Exceedance Ratio: {METRICS['exceedance_ratio']:.1f}%\")\n    print(f\"   L10 Sample: {METRICS['l10_sample_size']} prices from {l10_count} instances\")\n    \n    print(f\"\\nüí∞ PRIMARY STRATEGY - AGGRESSIVE:\")\n    print(f\"   Long Strike:  ${long_strike:.2f} (BUY)\")\n    print(f\"   Short Strike: ${short_strike:.2f} (SELL)\")\n    print(f\"   Spread Width: ${spread_width:.2f}\")\n    print(f\"   Max Profit: ${spread_width:.2f} per contract\")\n    print(f\"   Expiration: {expiration_date.strftime('%Y-%m-%d')}\")\n    print(f\"   Target: ${pattern_l10_median_price:.2f} ({upside_to_l10:+.1f}% from entry)\")\n    \n    print(f\"\\nüí° ALTERNATIVE - CONSERVATIVE:\")\n    print(f\"   Long Strike:  ${conservative_long:.2f} (BUY)\")\n    print(f\"   Short Strike: ${conservative_short:.2f} (SELL)\")\n    print(f\"   Rationale: Lower strikes for higher probability of profit\")\n    \n    print(f\"\\nüéØ PRICE LEVELS:\")\n    print(f\"   Current Entry: ${entry_price:.2f}\")\n    print(f\"   L10 Target: ${pattern_l10_median_price:.2f}\")\n    print(f\"   L10 Terminal Median: ${METRICS['terminal_median_price']:.2f}\")\n    print(f\"   Global Baseline: ${METRICS['baseline_median']:.2f}\")\n    \n    print(f\"\\nüìä EDGE CALCULATION:\")\n    print(f\"   L10 shows {METRICS['delta_pct']:.1f}% median improvement over global baseline\")\n    print(f\"   Historical probability: ~{METRICS['exceedance_ratio']:.0f}% of L10 reaching above entry\")\n    print(f\"   Exploits positive delta in L10 price density dynamics\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"TRADE MANAGEMENT:\")\n    print(\"‚Ä¢ Enter when spread offers 2:1+ reward:risk\")\n    print(\"‚Ä¢ Target 50-80% max profit (close early)\")\n    print(\"‚Ä¢ Stop loss: -50% of debit paid\")\n    print(\"‚Ä¢ Monitor for pattern invalidation\")\n    print(\"=\" * 60)\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"ENOMOTO BARCHART METHODOLOGY APPLIED:\")\n    print(\"=\" * 60)\n    print(\"‚úì Uses LAST 10 MATCHES (L10) for pattern-specific analysis\")\n    print(\"‚úì Uses ALL weeks 1-10 (not just week 10)\")\n    print(\"‚úì Uses ACTUAL historical prices (no projection bias)\")\n    print(\"‚úì Creates BIMODAL GMM on combined baseline + L10 data\")\n    print(\"‚úì Barchart-style visualization: blue baseline, green L10, red GMM\")\n    print(f\"‚úì L10 sample: {METRICS['l10_sample_size']} prices from {l10_count} instances\")\n    print(\"=\" * 60)\n    \nelif METRICS and METRICS['delta_pct'] < 0:\n    print(\"=\" * 60)\n    print(f\"NO BULLISH TRADE - Negative Delta\")\n    print(\"=\" * 60)\n    print(f\"L10 shows {METRICS['delta_pct']:.2f}% negative delta\")\n    print(f\"L10 median (${METRICS['pattern_l10_median']:.2f}) below global median (${METRICS['baseline_median']:.2f})\")\n    print(\"\\nConsider: Bearish strategies or avoid this trade\")\n    \nelse:\n    print(\"\\n‚ö†Ô∏è  Insufficient L10 pattern data for strategy recommendation\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}